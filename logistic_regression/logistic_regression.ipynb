{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: Theta = [ 0.          0.0025     -0.00916667], loss = 0.6931471805599453\n",
      "iteration 1: Theta = [ 7.98469259e-05  5.19198067e-03 -1.78976565e-02], loss = 0.6842951082769672\n",
      "iteration 2: Theta = [ 0.00023353  0.00806057 -0.026223  ], loss = 0.6760998377452818\n",
      "iteration 3: Theta = [ 0.00045541  0.01109139 -0.03417093], loss = 0.668476830969379\n",
      "iteration 4: Theta = [ 0.00074022  0.014271   -0.04176787], loss = 0.6613526643315134\n",
      "iteration 5: Theta = [ 0.00108305  0.01758691 -0.0490385 ], loss = 0.654663724600615\n",
      "iteration 6: Theta = [ 0.00147934  0.0210275  -0.05600577], loss = 0.6483549992830944\n",
      "iteration 7: Theta = [ 0.00192486  0.02458201 -0.06269101], loss = 0.6423789760518801\n",
      "iteration 8: Theta = [ 0.00241569  0.02824049 -0.069114  ], loss = 0.6366946559267823\n",
      "iteration 9: Theta = [ 0.00294818  0.03199371 -0.07529306], loss = 0.631266678248786\n",
      "iteration 10: Theta = [ 0.00351898  0.03583314 -0.08124515], loss = 0.6260645514013039\n",
      "iteration 11: Theta = [ 0.00412499  0.03975091 -0.08698597], loss = 0.6210619809536838\n",
      "iteration 12: Theta = [ 0.00476332  0.04373975 -0.09253001], loss = 0.6162362858698359\n",
      "iteration 13: Theta = [ 0.00543131  0.04779292 -0.09789071], loss = 0.6115678932142755\n",
      "iteration 14: Theta = [ 0.00612653  0.05190423 -0.10308045], loss = 0.6070399020928682\n",
      "iteration 15: Theta = [ 0.00684668  0.05606794 -0.10811072], loss = 0.602637708171062\n",
      "iteration 16: Theta = [ 0.00758968  0.06027875 -0.11299211], loss = 0.5983486808717815\n",
      "iteration 17: Theta = [ 0.00835358  0.06453177 -0.11773444], loss = 0.5941618861718191\n",
      "iteration 18: Theta = [ 0.00913658  0.06882246 -0.12234678], loss = 0.590067848728257\n",
      "iteration 19: Theta = [ 0.00993704  0.07314666 -0.12683754], loss = 0.5860583478386312\n",
      "iteration 20: Theta = [ 0.0107534   0.07750049 -0.13121449], loss = 0.582126242450267\n",
      "iteration 21: Theta = [ 0.01158426  0.08188038 -0.13548484], loss = 0.5782653210765033\n",
      "iteration 22: Theta = [ 0.01242828  0.08628302 -0.13965527], loss = 0.5744701730484456\n",
      "iteration 23: Theta = [ 0.01328426  0.09070536 -0.14373197], loss = 0.5707360780326708\n",
      "iteration 24: Theta = [ 0.01415106  0.09514456 -0.14772067], loss = 0.5670589111826833\n",
      "iteration 25: Theta = [ 0.01502763  0.099598   -0.15162671], loss = 0.5634350616707089\n",
      "iteration 26: Theta = [ 0.01591301  0.10406327 -0.15545502], loss = 0.559861362672913\n",
      "iteration 27: Theta = [ 0.0168063   0.10853811 -0.1592102 ], loss = 0.5563350311615325\n",
      "iteration 28: Theta = [ 0.01770665  0.11302046 -0.16289652], loss = 0.5528536160975874\n",
      "iteration 29: Theta = [ 0.01861331  0.11750838 -0.16651793], loss = 0.5494149538231348\n",
      "iteration 30: Theta = [ 0.01952555  0.12200009 -0.17007813], loss = 0.5460171296272921\n",
      "iteration 31: Theta = [ 0.0204427   0.12649396 -0.17358054], loss = 0.5426584446097193\n",
      "iteration 32: Theta = [ 0.02136415  0.13098845 -0.17702835], loss = 0.539337387092652\n",
      "iteration 33: Theta = [ 0.02228933  0.13548214 -0.18042453], loss = 0.5360526079411367\n",
      "iteration 34: Theta = [ 0.0232177   0.13997374 -0.18377186], loss = 0.5328028992436377\n",
      "iteration 35: Theta = [ 0.02414877  0.14446204 -0.18707289], loss = 0.5295871758840226\n",
      "iteration 36: Theta = [ 0.02508208  0.14894591 -0.19033004], loss = 0.52640445960316\n",
      "iteration 37: Theta = [ 0.02601722  0.15342432 -0.19354554], loss = 0.5232538652056975\n",
      "iteration 38: Theta = [ 0.02695377  0.15789632 -0.19672149], loss = 0.5201345886165251\n",
      "iteration 39: Theta = [ 0.02789139  0.16236101 -0.19985982], loss = 0.5170458965332159\n",
      "iteration 40: Theta = [ 0.02882972  0.16681759 -0.20296236], loss = 0.5139871174564513\n",
      "iteration 41: Theta = [ 0.02976846  0.1712653  -0.20603081], loss = 0.5109576339109805\n",
      "iteration 42: Theta = [ 0.03070731  0.17570344 -0.20906674], loss = 0.5079568756958032\n",
      "iteration 43: Theta = [ 0.031646    0.18013138 -0.21207165], loss = 0.5049843140246594\n",
      "iteration 44: Theta = [ 0.03258428  0.18454852 -0.21504692], loss = 0.5020394564371046\n",
      "iteration 45: Theta = [ 0.03352191  0.18895432 -0.21799383], loss = 0.4991218423769159\n",
      "iteration 46: Theta = [ 0.03445869  0.19334828 -0.2209136 ], loss = 0.4962310393487208\n",
      "iteration 47: Theta = [ 0.03539441  0.19772994 -0.22380737], loss = 0.4933666395758905\n",
      "iteration 48: Theta = [ 0.03632889  0.20209887 -0.22667619], loss = 0.49052825709318965\n",
      "iteration 49: Theta = [ 0.03726196  0.2064547  -0.22952105], loss = 0.4877155252166754\n",
      "iteration 50: Theta = [ 0.03819345  0.21079708 -0.23234289], loss = 0.4849280943410818\n",
      "iteration 51: Theta = [ 0.03912323  0.21512566 -0.23514258], loss = 0.4821656300216112\n",
      "iteration 52: Theta = [ 0.04005115  0.21944018 -0.23792092], loss = 0.479427811302811\n",
      "iteration 53: Theta = [ 0.0409771   0.22374035 -0.2406787 ], loss = 0.47671432926219276\n",
      "iteration 54: Theta = [ 0.04190095  0.22802593 -0.24341662], loss = 0.4740248857405441\n",
      "iteration 55: Theta = [ 0.0428226   0.23229671 -0.24613535], loss = 0.4713591922345999\n",
      "iteration 56: Theta = [ 0.04374196  0.23655249 -0.24883554], loss = 0.46871696893095377\n",
      "iteration 57: Theta = [ 0.04465893  0.24079309 -0.25151778], loss = 0.4660979438628654\n",
      "iteration 58: Theta = [ 0.04557344  0.24501836 -0.25418262], loss = 0.4635018521740332\n",
      "iteration 59: Theta = [ 0.0464854   0.24922814 -0.25683059], loss = 0.4609284354754846\n",
      "iteration 60: Theta = [ 0.04739475  0.25342232 -0.25946218], loss = 0.4583774412835471\n",
      "iteration 61: Theta = [ 0.04830143  0.25760078 -0.26207785], loss = 0.45584862252842795\n",
      "iteration 62: Theta = [ 0.04920537  0.26176344 -0.26467805], loss = 0.4533417371242982\n",
      "iteration 63: Theta = [ 0.05010653  0.2659102  -0.26726318], loss = 0.4508565475929511\n",
      "iteration 64: Theta = [ 0.05100485  0.270041   -0.26983364], loss = 0.448392820734139\n",
      "iteration 65: Theta = [ 0.0519003   0.27415577 -0.27238978], loss = 0.44595032733657797\n",
      "iteration 66: Theta = [ 0.05279283  0.27825448 -0.27493196], loss = 0.4435288419243904\n",
      "iteration 67: Theta = [ 0.05368242  0.28233708 -0.2774605 ], loss = 0.44112814253442734\n",
      "iteration 68: Theta = [ 0.05456902  0.28640354 -0.2799757 ], loss = 0.43874801052049905\n",
      "iteration 69: Theta = [ 0.05545261  0.29045384 -0.28247786], loss = 0.43638823038105423\n",
      "iteration 70: Theta = [ 0.05633317  0.29448797 -0.28496725], loss = 0.43404858960729104\n",
      "Final weights: [ 0.05633317  0.29448797 -0.28496725]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#define the sigmoid function \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#loss function log likelihood \n",
    "def compute_loss(y, hx):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    hx = np.clip(hx, 1e-10, 1-1e-10)\n",
    "    return -np.mean(y*np.log(hx) + (1-y) * np.log(1 - hx))\n",
    "\n",
    "#gradient of the loss \n",
    "def compute_gradient(x, y, hx):\n",
    "    return np.dot(x.T, (hx - y)) / y.shape[0]\n",
    "\n",
    "x = np.array([\n",
    "    [2,1],\n",
    "    [3,2],\n",
    "    [4,4],\n",
    "    [1,5],\n",
    "    [2,6],\n",
    "    [3,7]\n",
    "])\n",
    "y = np.array([1,1,1,0,0,0])\n",
    "\n",
    "#logistic regression model \n",
    "def logistic_regression(x, y, learning_rate=0.01, num_iterations=71):\n",
    "    #add intercept term to x \n",
    "    #augmentation is taking place here \n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "    #initialize weights\n",
    "    weights = np.zeros(x.shape[1])\n",
    "\n",
    "    #gradient descent \n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(x, weights)\n",
    "        hx = sigmoid(z)\n",
    "        loss = compute_loss(y, hx)\n",
    "        gradient = compute_gradient(x, y, hx)\n",
    "        weights -= learning_rate * gradient\n",
    "\n",
    "        print(f'iteration {i}: Theta = {weights}, loss = {loss}')\n",
    "\n",
    "    return weights\n",
    "\n",
    "weights = logistic_regression(x, y)\n",
    "print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib as plt \n",
    "\n",
    "#define the sigmoid function \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function log likelihood \n",
    "def compute_loss(y,hx):\n",
    "    return -np.mean(y*np.log(hx) + (1-y) * np.log(1 - hx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient of the loss \n",
    "def compute_gradient(x,y,hx):\n",
    "    return np.dot(x.T, (hx - y )) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [2,1],\n",
    "    [3,2],\n",
    "    [4,4],\n",
    "    [1,5],\n",
    "    [2,6],\n",
    "    [3,7]\n",
    "]\n",
    ")\n",
    "y = np.array([1,1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model \n",
    "def logistic_regression(x,y,learning_rate=0.01,num_iterations=10):\n",
    "    #add intercept term to x \n",
    "    #augmentation is taking place here \n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "    #initialize weights\n",
    "    weights = np.zeros(x.shape[1])\n",
    "\n",
    "    #gradient descent \n",
    "    for i in range(num_iterations):\n",
    "        z= np.dot(x,weights)\n",
    "        hx = sigmoid(z)\n",
    "        loss = compute_loss(y,hx)\n",
    "        gradient = compute_gradient(x,y,hx)\n",
    "        weights -= learning_rate * gradient\n",
    "\n",
    "        print(f'iteration {i}: Theta = {weights},loss = {loss}')\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: Theta = [ 0.          0.0025     -0.00916667],loss = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
